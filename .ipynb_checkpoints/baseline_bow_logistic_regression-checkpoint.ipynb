{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import util\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So first lets just get the data in an easy to work with format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = util.get_full_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape yelp  data:  (1000, 3)\n",
      "shape amazon  data:  (1000, 3)\n",
      "shape imdb  data:  (748, 3)\n"
     ]
    }
   ],
   "source": [
    "for source in df['source'].unique():\n",
    "    \n",
    "    df_source = df[df['source'] == source]\n",
    "    print('shape',source,' data: ', df_source.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so we've got the dataset in a data frame. Great. Now to decide on the method. In this example we are going to use a BOW approach (not the best, but a good fundamental) to do this we can use scikit learns CountVectorizer which creates a vocabulary that we will use to build feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'John': 0, 'likes': 4, 'ice': 3, 'cream': 1, 'hates': 2}\n"
     ]
    }
   ],
   "source": [
    "sentences = ['John likes ice cream','John hates ice cream']\n",
    "vectorizer = CountVectorizer(min_df=0,lowercase=False)\n",
    "vectorizer.fit(sentences)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now that we have a vocabulary we can use transfrom to get feature vecots to use in training. the resulting vector is the version of our vocabulary vector with each indecy representing that words frequency in a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(sentences).toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's time to split our set for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 750 \n",
      "Testing set: 250 \n"
     ]
    }
   ],
   "source": [
    "df_yelp = df[df['source'] == 'yelp']\n",
    "sentences = df_yelp['sentence'].values\n",
    "y = df_yelp['label'].values\n",
    "\n",
    "sentences_train,sentences_test,y_train,y_test = train_test_split(sentences,y,test_size=0.25,random_state=1000)\n",
    "\n",
    "print('Training set: {} '.format(len(sentences_train)))\n",
    "print('Testing set: {} '.format(len(sentences_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to vectorize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<750x1714 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create our vocabulary using only the training set\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(sentences_train)\n",
    "\n",
    "#then we create our feature vectors for every sentence. NOTE: scikit takes care of lots of preprocessing for us by\n",
    "#tokenizing sentences (removing punctuation, special characters....)\n",
    "#this is one of our HYPER PERAMETERS, we can change how we tokenize\n",
    "X_train = vectorizer.transform(sentences_train)\n",
    "X_test =  vectorizer.transform(sentences_test)\n",
    "\n",
    "#for these reviews the vocabulary is 1714 words which will mean that every feature vector is that long as well (not hot)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elidangerfield/anaconda3/envs/firstML/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#the model we will use is Logistic Regression, a simple classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)\n",
    "score = classifier.score(X_test,y_test)\n",
    "\n",
    "print('Accuracy: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for  yelp  data:  0.938\n",
      "Accuracy for  amazon  data:  0.736\n",
      "Accuracy for  imdb  data:  0.660427807486631\n"
     ]
    }
   ],
   "source": [
    "# we got a lofty 79%, not terrible but not amazing\n",
    "# now lets see how our model matches up against the oterh data sets\n",
    "\n",
    "for source in df['source'].unique():\n",
    "    \n",
    "    df_source = df[df['source'] == source]\n",
    "    sentences = df_source['sentence'].values\n",
    "    y = df_source['label'].values\n",
    "    X =  vectorizer.transform(sentences)\n",
    "    \n",
    "    score = classifier.score(X,y)\n",
    "    print('Accuracy for ',source,' data: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### okay so it makes sense to get 94% on the set that we used 75% of it for but as you can see it do so hot on the next two sets. However, this sets the baseline to beat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNIST_CNN",
   "language": "python",
   "name": "mnist_cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
